---
title: "Neural networks"
author: "Sangyeol_lee"
date: "Wednesday, May 06, 2015"
output: html_document
---

###**제5장 신경망** 
 + 생물학적 신경망의 구조로부터 착안된 학습 알고리즘.
 + 입력값과 출력값간의 매우 복잡한 형태의 비선형 모형화 방법.
 + 1세대 퍼셉트론 -> 2세대 다층퍼셉트론(역전파 알고리즘) -> 3세대 딥러닝 

> ####**개요**
 + 인공신경망의 기원은 1958년에 Rosenblatt가 제안한 퍼셉트론이 시작.
 + n개의 input과 1개의 output에 대하여 각각의 input의 weight를 Wi라고 한 후 퍼셉트론을 수식으로 나타내면
 $$y = \varphi \left ( \sum_{i=1}^{n}\omega i xi + b \right ) $$ 

![그림참고](http://www.codeproject.com/KB/recipes/NeuralNetwork_1/NN2.png)

### sigmoid function $$ f(x) = \frac{1}{1 + e^{-\beta x}} $$
![그림참고](http://www.codeproject.com/KB/recipes/NeuralNetwork_1/Sigmoid.png)

### step funtion $$ f(x) = 0 if x < 0, 1 if x >= 0 $$
![그림참고](http://www.codeproject.com/KB/recipes/NeuralNetwork_1/Step.png)

### XOR PROBLEM
![그림참고](http://toritris.weebly.com/uploads/1/4/1/3/14134854/416470.jpg?323)

> ### **다층퍼셉트론(Multilayer Perceptron)**

![그림참고](http://www.mu-sigma.com/analytics/images/cafe_cerebral/Multilayer%20perceptron.gif)

> ### feed forward propagation (전방 학습법)
    입력층, 중간층, 출력층. 예제 : 치즈에 대한 감정 점수 예제 x1 지방점수 x2 염분점수 y는 1 치즈를 선호, 0은 선호하지 않음

---
```{r}
example5_1 <- data.frame(no = 1:6, x1 = c(0.2,0.1,0.2,0.2,0.4,0.3), x2=c(0.9,0.1,0.4,0.5,0.5,0.8), y=c(1,0,0,0,1,1))
example5_1

#중간층 1 
middle1 <- 1 / (1 + exp(-(-0.3 + example5_1[1,2] * 0.05 + example5_1[1,3] * 0.01)))
middle1

#중간층 2
middle2 <- 1 / (1 + exp(-(0.2 + example5_1[1,2] * -0.01 + example5_1[1,3] * 0.03)))
middle2

#중간층 3
middle3 <- 1 / (1 + exp(-(0.05 + example5_1[1,2] * 0.02 + example5_1[1,3] * -0.01)))
middle3

#출력층
1 / (1+ exp(-(0.015 + middle1 * 0.01 + middle2 * 0.05 + middle3 * 0.0015)))
#0.5119561 > 0.5보다 크기 때문에 1로 분류
```

###back propagation (역전파 학습법)
   + R(theta)은 회귀/분류 문제에 따라 오차제곱합,Deviance 통계량으로 구하는데 은닉노드에 대한 선형 로지스틱 회귀로 보면 비선형의 문제가 발생하여 전역 최소값(global minimal)을 구할 수 없다.
   + 그대신 국소 최소값(Local minimal)을 얻기 위하여 역전파라 불리는 기울기 강하 알고리즘(gradient descent)을 사용한다. 강하 알고리즘은 결국 에러를 미분하여 weight를 업데이트 하는 방법이고 역전파 학습법은 은닉 노드에서 역으로 내려오면서 각 노드마다 에러를 구하고 에러를 인풋만큼 곱해서 차곡차곡 역순으로 에러를 학습하는 것.
   +  $$ \bigtriangledown E[\vec{w}] = [ \frac{\varrho E}{\varrho w_{o}} , \frac{\varrho E}{\varrho w_{1}} ,..., \frac{\varrho E}{\varrho w_{n}}] $$
   + $$ \bigtriangleup \vec{w} = -\eta \bigtriangledown E[\vec{w}] $$ 
   + $$ \bigtriangleup w_{i} = -\eta \frac{\varrho E}{\varrho w_{i}} $$
   
![그림참고](http://cfile3.uf.tistory.com/image/25343A4B534CD49A1FB2AC)

   + theta : 연결강도, W 가중치, 0.5 : 학습률
   + $$ err^{_{k}} = \hat{y}^{_{k}}(1 - \hat{y}^{_{k}})(yk - \hat{y}^{_{k}}) $$
   + $$ (0.5119)(1-0.5119)(1-0.5119) = 0.1219 $$
   + $$ \theta ^{_{6}} = -0.015 + (0.5)(0.1219) = 0.0591215 $$
   + $$ w_{3,6} = 0.01 + (0.5)(0.1219) = 0.07095 $$
   + $$ w_{4,6} = 0.05 + (0.5)(0.1219) = 0.11095 $$
   + $$ w_{5,6} = 0.015 + (0.5)(0.1219) = 0.0627785 $$
   + 이런식으로 모든 관측값이 사용될 때까지 신경망에 입력된 후에 갱신. 이것을 한 세대(epoch)이라고 하면 반복(iteration)이라고 부른다. 보통 여러번의 반복을 시도.
   + 배치 학습모드(batch learning mode)에서는 연결강도의 갱신이 실행되기 전에 전체 학습세트가 신경망에 입력됨. 평균 방향 방향이 안정적
   + 온라인 학습모드(online learning mode)에서는 하나씩 신경망에 투입하여 가중치 추정값을 매번 조정함. 방향이 중구난방이지만 Local Optima를 벗어날 수 있는 장점.
   + 확률적 학습모드(probabilistic learning mode)에서는 온라인 학습모드의 변형으로 신경망에 투입되는 관측값의 순서가 랜덤함.
   + 학습이 멈추는 조건
       1. 새로운 연결강도가 이전 반복에서 얻어지는 것보다 조금만 차이가 날때, 
       2. 오분류율이 요구된 목표치에 도달했을 때 
       3. 반복 실행회수의 한계에 도달했을 때

---

###신경망 모형 구축시 고려사항
#### *입력변수*
   + 입력자료를 선택하는 문제
       +범주형 입력변수가 모든 범주에서 일정빈도 이상의 값을 갖음, 연속형 입력변수값들의 범위가 변수간 차이가 없음.
       +입력변수의 수가 너무 적거나 많지 않고, 범주형 출력값의 각 범주의 빈도가 비슷한 자료.
   + 연속형 입력변수의 변환 또는 변주화
       +평균을 중심으로 대칭이 아닌 경우 좋지 않은 결과 -> 로그 변환
       +연속형 변수를 범주화하는 것
   + 새로운 변수 생성
       +원 입력변수들을 조합하여 새로운 변수를 만든 후 생성된 변수를 입력변수로 사용
   + 범주형 입력변수의 가변수화
       +남자와 여자를 0과 1로 가변수화 하는 것과 -1과 1로 가변수화 하는 것은 그 결과가 틀려질 수 있다는 것.

#### *초기치와 다중 최소값 문제*
   + 가중치 초기치가 0의 가까울수록 시그모이드 함수가 선형이 되고 신경망 모형이 근사적인 선형 모형
   + 가중치가 증가할수록 비선형 모형. 신경망에서의 비용함수는 비볼록함수이고 여러개의 국소 최소값들을 가진다. (Local minima)
   + 해결책은 랜덤하게 선택된 여러개의 초기치에 대해 신경망을 적합한 후 얻은 해들을 비교하여 가장 오차가 작은 값을 선택하거나 예측값의 평균을 구하여 최종예측치.

#### *은닉층과 은닉노드의 수*
   + 은닉층의 수와 은닉노드의 수를 결정하는 것이 주요 문제
   + 너무 많으면 추정할 모수의 가중치들이 너무 많아져서 과대적합 / 너무 적으면 반대로 과소적합
   + 은닉층의 수는 기본적으로 자료의 성격이나 상황에 따라 결정.
   + 은닉층이 하나인 범용 근사자. 하나로 두고 노드의 숫자를 적절히 큰 값으로 두고 가중치 감소(weight decay)라는 모수에 대한 벌점화를 적용하는 것이 좋다.

#### *과대적합 문제*
   + 과대적합을 피하기 위한 방법으로는 알고리즘의 조기종료와 가중치 감소 기법
   + 조기종료는 모형을 적합하는 과정에서 검증오차가 증가하기 시작하면 반복을 중지하는 방법
   + 선형모형의 능형회귀와 유사한 가중치 감소라는 벌점화 기법.
   + $$ R(\theta ) + \lambda J(\theta ) $$
   + $$ J(\theta ) = \sum_{k,m}\beta ^2{_{km}^{}} + \sum_{m,l}\alpha  ^2{_{m,l}^{}}^{} $$
   + R(theta)는 회귀문제에서는 오차제곱합 // 분류문제에서는 deviance
   