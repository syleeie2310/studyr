
# Scala Study 5 Naive Bayes Classifiers (psygrammer Sangyeol Lee) 08/08(토)
## Probabilistic graphical models
- Directed graphs such as Bayesian networks
- Undirected graphs such as conditional random fields (refer to the Conditional random fields section in chapter 7, Sequential Data Models)

### Directed graphical models are directed acyclic graphs that have been introduced to:
- Provide a simple way to visualize a probabilistic model
- Describe the conditional dependence (or independence) between variables
- Represent statistical inference in terms of graphical manipulation

>###*Bayes Theorem*
![BayesTheorem](http://www.lancs.ac.uk/~jamest/Group/images/bayesthm.jpg)

>###*Examples of probabilistic graphical models*
![PGM](naive.jpg)

###A Real-world Bayesian network
1. A fire may generate smoke.
2. Smoke may trigger an alarm.
3. A depleted battery may trigger an alarm.
4. The alarm may alert the homeowner.
5. The alarm may alert the fire department.

>###*A Bayesian network for smokre detectors*
![PGM](smoke.jpg)

## Naive Bayes Classifiers
### Introducing the multinomial Naive Bayes
Let's consider the problem of how to predict change in interest rates. The first step is to list the factors that potentially may trigger or cause an increase or decrease in the interest rates. For the sake of illustrating Naïve Bayes, we will select the consumer
price index (CPI), change in the Federal fund rate (FDF) and the gross domestic product (GDP) as a first set of features. The terminology is described in the Terminology section under Finances 101 in Appendix A, Basic Concepts.
(금리 변화 -> CPI(소비자 물가지수), FDF(연방기금금리), GDP(국내총생산)

두가지 가정
1. 금융 투자 전문가가 없음 (히스토리 데이터에 의존)
2. 1년 재무부 증권 증가의 수율(금리 변화)는 선택한 기능과의 강한 상관 관계가 있음

>###*The Naive Bayes model for predicting the Change in the yield of the 1-year T-bill*
![PGM](t-bill.jpg)

>###*Table*
![PGM](table.jpg)

>###*Table2*
![PGM](table2.jpg)

### Formalism
- Class prior probability or class prior is the probability of a class
- Likelihood is the probability of an observation given a class, also known as the probability of the predictor given a class
- Evidence is the probability of observations occurring, also known as the prior probability of the predictor
- Posterior probability is the probability of an observation x being in a given class

>###*Mathematical notation for the Naive Bayes model*
![PGM](formal.jpg)

>###*Frequentist perspective*
![PGM](result.jpg)

####This problem is not a good candidate for a Bayesian classification for two reasons:
- The training set is not large enough to compute accurate prior probabilities and generate a stable model; decades of quarterly GDP data is needed to train and validate the model
- The features have different rates of change, which predominately favor the feature with the highest frequency; in this case, the CPI


###The predictive model
- The closing price, pr(t), of the last trading session, t, is above or below the average closing price over the n previous trading days, [t-n, t]
- The volume of the last trading day, vl(t), is above or below the average volume of the n previous trading days
- The volatility on the last trading day, vt(t), is above or below the average volatility of the previous n trading days

![PGM](stock.jpg)

### The zero-frquency problem
![PGM](zero.jpg)


### Implementation
#### Software design
- A generic model, NaiveBayesModel, of the type Model, which is initialized through training during the instantiation of the class.
- A model for the binomial classification, BinNaiveBayesModel, which subclasses NaiveBayesModel. The model consists of a density function of the type Density, and a pair of positive and negative Likelihood instances. 
- A model for the multinomial classification MultiNaiveBayesModel.
- The predictive or classification routine is implemented as a data transformation extending the PipeOperator trait.
- The NaiveBayes classifier class has two parameters: a smoothing

>###*UML Class diagram*
![PGM](umlnaive.jpg)

###Training
- The number of occurrences k of this features for N > k observations in case of binary features or counters
- The mean value for all the observations for this features in the case of numeric or continuous features
- 가정 : 가격, 볼륨, 변동성이 조건부 독립임을 가정

####Likelihood class
- The label for the observation, label
- An array of tuple Laplace or Lidstone smoothed mean and standard deviation, muSigma
- The prior class prior that computes p(c)


    type Density = (Double*) => Double //1
    type XYTSeries = Array[(Double, Double)]
    val MINLOGARG = 1e-32
    val MINLOGVALUE = -MINLOGARG
    class Likelihood[T <% Double](val label: Int, val muSigma: XYTSeries,
    prior: Double) { //2
        def score(obs: Array[T], density: Density): Double =
         (obs, muSigma).zipped
                         .foldLeft(0.0)((post, xms) => {
                         val mean = xms._2._1
                         val stdDev = xms._2._2
                         val _obs = xms._1
            val prob = density(mean, stdDev, _obs)
            post + Math.log(if(prob< MINLOGARG) MINLOGVALUE else prob)
        }) + Math.log(prior) //3
    }


    defined [32mtype [36mDensity[0m
    defined [32mtype [36mXYTSeries[0m
    [36mMINLOGARG[0m: [32mDouble[0m = [32m1.0E-32[0m
    [36mMINLOGVALUE[0m: [32mDouble[0m = [32m-1.0E-32[0m
    defined [32mclass [36mLikelihood[0m


####The parameterized, view-bounded class, Likelihood, has two purposes:
- Define the model extracted from training (likelihood for each feature and the class prior) in the constructor (line 2)
- Compute the score of a new observation as part of the classification process score (line 3). The computation of the log of the likelihood uses a density method of the type Density, which is an argument of the score method. As seen in the next section, the density can be either a Gaussian or a Bernoulli distribution. The score method uses the Scala's zipped method to merge the observation values with the labeled output

####두 리스트를 순서쌍으로 묶기: zip, unzip
- zip 두 리스트를 인자로 받아서 순서쌍의 리스트로 만들며, 만약 인자 리스트 둘의 길이가 다르다면 길이가 긴 쪽의 남는 원소는 버린다.

The next step is to define the BinNaiveBayesModel model for a two-class classification scheme. The two-class model consists of the two Likelihood instances:

positives for the label UP (value==1) and negatives for the label DOWN (value==0). In order to make the model generic, we created NaiveBayesModel, an abstract class that can be extended as needed to support both the Binomial and Multinomial Naïve Bayes models, as follows:


    val abcde = List('a', 'b', 'c', 'd', 'e')
    abcde.indices zip abcde
    val zipped = abcde zip List(1, 2, 3)


    [36mabcde[0m: [32mList[0m[[32mChar[0m] = [33mList[0m([32m'a'[0m, [32m'b'[0m, [32m'c'[0m, [32m'd'[0m, [32m'e'[0m)
    [36mres1_1[0m: [32mcollection[0m.[32mimmutable[0m.[32mIndexedSeq[0m[([32mInt[0m, [32mChar[0m)] = [33mVector[0m(
      [33m[0m([32m0[0m, [32m'a'[0m),
      [33m[0m([32m1[0m, [32m'b'[0m),
      [33m[0m([32m2[0m, [32m'c'[0m),
      [33m[0m([32m3[0m, [32m'd'[0m),
      [33m[0m([32m4[0m, [32m'e'[0m)
    )
    [36mzipped[0m: [32mList[0m[([32mChar[0m, [32mInt[0m)] = [33mList[0m(
      [33m[0m([32m'a'[0m, [32m1[0m),
      [33m[0m([32m'b'[0m, [32m2[0m),
      [33m[0m([32m'c'[0m, [32m3[0m)
    )



    type XY = (Double, Double)
    type XYTSeries = Array[(Double, Double)]
    type DMatrix[T] = Array[Array[T]]
    type DVector[T] = Array[T]
    type DblMatrix = DMatrix[Double]
    type DblVector = Array[Double]


    defined [32mtype [36mXY[0m
    defined [32mtype [36mXYTSeries[0m
    defined [32mtype [36mDMatrix[0m
    defined [32mtype [36mDVector[0m
    defined [32mtype [36mDblMatrix[0m
    defined [32mtype [36mDblVector[0m



    class XTSeries[T](label: String, arr: Array[T])


    defined [32mclass [36mXTSeries[0m



    class XTSeries[T](label: String, arr: Array[T]) { // 1
      def apply(n: Int): T = arr.apply(n)
        
      @implicitNotFound("Undefined conversion to DblVector") // 2
      def toDblVector(implicit f: T=>Double):DblVector =arr.map(f(_))
    
      @implicitNotFound("Undefined conversion to DblMatrix") // 2
      def toDblMatrix(implicit fv: T => DblVector): DblMatrix = arr.map(
    fv( _ ) )
    
      def + (n: Int, t: T)(implicit f: (T,T) => T): T = f(arr(n), t)
    
      def head: T = arr.head //3
      def drop(n: Int):XTSeries[T] = XTSeries(label,arr.drop(n))
      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,
    arr.map( x =>f(x)))
      def foreach( f: T => Unit) = arr.foreach(f) //3
      def sortWith(lt: (T,T)=>Boolean):XTSeries[T] = XTSeries[T](label,
    arr.sortWith(lt))
      def max(implicit cmp: Ordering[T]): T = arr.max //4
    def min(implicit cmp: Ordering[T]): T = arr.min
    }


    Compilation Failed

    Main.scala:76: not found: type implicitNotFound

      @implicitNotFound("Undefined conversion to DblVector") // 2

       ^

    Main.scala:79: not found: type implicitNotFound

      @implicitNotFound("Undefined conversion to DblMatrix") // 2

       ^

    Main.scala:86: not found: value XTSeries

      def drop(n: Int):XTSeries[T] = XTSeries(label,arr.drop(n))

                                     ^

    Main.scala:87: not found: type ClassTag

      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,

                 ^

    Main.scala:87: not found: value XTSeries

      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,

                                                     ^

    Main.scala:90: not found: value XTSeries

      def sortWith(lt: (T,T)=>Boolean):XTSeries[T] = XTSeries[T](label,

                                                     ^



    val numFeatures = 4


    [36mnumFeatures[0m: [32mInt[0m = [32m4[0m



    abstract class NaiveBayesModel [T <% Double](density: Density) {
        def classify(values: DblVector): Int
    }


    defined [32mclass [36mNaiveBayesModel[0m



    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 
    {
        override def classify(x: Array[T]): Int = if (positives.score(x,density) > negatives.score(x,density)) 1 else 0
    }


    Compilation Failed

    Main.scala:81: class Likelihood takes type parameters

                    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 

                                                                      ^

    Main.scala:81: class Likelihood takes type parameters

                    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 

                                                                                             ^


###multinomial Naive Bayes model
The multinomial Naïve Bayes model differs from the binomial version in the following ways:
- The likelihood is defined as a list, likelihoodXs (one likelihood per class)
- The runtime classification sorts the class by the log likelihood (sortWith), selects the class with the highest score, and returns the class ID

The validate method takes a labeled time series xt as an array of tuples (observation, class label) and the tpClass index that contains the true positives (that is, increase in the stock price) outcome. The method returns an F1-measure. Besides inheriting the Supervised trait, the NaiveBayes class inherits the PipeOperator trait so that it can be integrated into a generic workflow as one of the computation units.

The attributes of the multinomial Naïve Bayes are as follows:
- The smoothing formula (Laplace, Lidstone, and so on): smoothing
- The labeled training set defined as a time series: xt
- The probability density function: density

>###*F1-Measure*
![PGM](f1.jpg)

http://blog.naver.com/kmkim1222?Redirect=Log&logNo=220106232149 (혼동행렬)


    class MultiNaiveBayesModel[T <% Double](likelihoodXs:List[Likelihood[T]], density: Density) extends NaiveBayesModel[T](density) 
    {
        override def classify(x: Array[T]): Int = likelihoodXs.sortWith( (p1,p2) => p1.score(x, density) >
    p2.score(x, density)).head.label
    }


    Compilation Failed

    Main.scala:81: class MultiNaiveBayesModel needs to be abstract, since method classify in class NaiveBayesModel of type (values: cmd6.INSTANCE.$ref$cmd3.DblVector)Int is not defined

    (Note that cmd6.this.$ref$cmd3.DblVector does not match Array[T]: their type parameters differ)

                    class MultiNaiveBayesModel[T <% Double](likelihoodXs:List[Likelihood[T]], density: Density) extends NaiveBayesModel[T](density) 

                          ^

    Main.scala:83: method classify overrides nothing.

    Note: the super classes of class MultiNaiveBayesModel contain the following, non final members named classify:

    def classify(values: cmd6.INSTANCE.$ref$cmd3.DblVector): Int

        override def classify(x: Array[T]): Int = likelihoodXs.sortWith( (p1,p2) => p1.score(x, density) >

                     ^



    trait Supervised[T] {
        def validate(xt: XTSeries[(Array[T],Int)], tpClass:Int): Double
    }


    defined [32mtrait [36mSupervised[0m



    Class NaiveBayes[T <% Double](smoothing: Double, xt:XTSeries[(Array[T], Int)], density: Density) extends PipeOperator[XTSeries[Array[T]], Array[Int]] with Supervised[T] 
    {
        val model = BinNaiveBayesModel[T](train(1),train(0),density) //1
        def train(label:Int)(implicit f: Array[T] => DblVector):Likelihood[T] = { //2
            val xi = xt.toArray
            val values= xi.filter( _._2 == label).map(x => f(x._1) )
            val dim = xi(0)._1.size
            val vt = XTSeries[DblVector](values.toArray) //3
            val muStdDev = statistics(vt).map(stat => (stat.lidstoneMean(smoothing, dim), stat.stdDev)) Likelihood(label, muStdDev, values.size.toDouble/xi.size) //4
    }


    SyntaxError: found " <% Double](smoothin", expected ("," ~ Type | "]") in

    Class NaiveBayes[T <% Double](smoothing: Double, xt:XTSeries[(Array[T], Int)], density: Density) extends PipeOperator[XTSeries[Array[T]], Array[Int]] with Supervised[T] 

                      ^


###Classification
The likelihood and class prior that have been computed through training is used for validating the model and classifying new observations.

The score represents the log of likelihood estimate (or the posterior probability), which is computed as the summation of the log of the Gaussian distribution using the mean and standard deviation, extracted from the training phase and the log of the likelihood class.

The Naïve Bayes classification using Gaussian distribution is illustrated using two classes, C1 and C2, and a model with two features (x and y):

![PGM](gaussian.jpg)
>###*Naïve Bayes classification using Gaussian density*
![PGM](gaussian2.jpg)

https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98 (위키피디아 나이브 베이지 분류)


    object Stats {
        final val INV_SQRT_2PI = 1.0/Math.sqrt(2.0*Math.PI)
        def gauss(mu: Double, sigma: Double, x:Double) : Double = {
         val y = x - mu
         INV_SQRT_2PI/sigma * Math.exp(-0.5*y*y/sigma*sigma)
    }
    def gauss(x: Double*): Double = gauss(x(0), x(1), x(2))
    }


    defined [32mobject [36mStats[0m


Finally, the classification method is implemented as the pipe operator |> of the NaiveBayes class. The classification model and the density function are provided at runtime as attributes of the class:


    def |> : PartialFunction[XTSeries[Array[T]], Array[Int]] = {
        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}


    Compilation Failed

    Main.scala:80: not found: type T

                    def |> : PartialFunction[XTSeries[Array[T]], Array[Int]] = {

                                                            ^

    Main.scala:81: not found: type T

        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}

                                ^

    Main.scala:81: not found: value model

        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}

                                                                    ^


###Labeling
The most critical element in the training of a supervised learning algorithm is the creation of labeled data. Fortunately, in this case, the label (or expected class) can be automatically generated. The objective is to predict the direction of the price of a stock for the next trading day, taking into account the average price, volume, and volatility over the last n days.

첫 번째 단계는 일일 및 주간 종가와 2000년 1월 1일 및 2014년 12월 31일의 기간 동안 각 주식의 평균 가격, 볼륨 및 변동성을 추출하는 것입니다. 이제 [TN, T] 창 이러한 평균을 계산하는 단순 이동 평균을 사용하자. 다음과 같이 첫째, 추출 기능은 toDouble와 % 연산자를 데이터 추출에 설명 및 데이터는 부록 A, 기본 개념 섹션에서 소스를 사용하여 각 거래일의 종가, 높은, 낮은 가격, 그리고 볼륨을 추출합니다

- Extractor ((closing, high, low) prices and volume for each trading day)
- Window period 따른 단순평균이동법을 사용하여 각 주식의 통계량을 뽑음


    val numbers = List(1, 2, 3, 4)
    numbers.drop(2)


    [36mnumbers[0m: [32mList[0m[[32mInt[0m] = [33mList[0m([32m1[0m, [32m2[0m, [32m3[0m, [32m4[0m)
    [36mres10_1[0m: [32mList[0m[[32mInt[0m] = [33mList[0m([32m3[0m, [32m4[0m)



    val extractor = toDouble(CLOSE) //stock closing price
                :: ratio(HIGH, LOW) //volatility (HIGH-LOW)/HIGH
                :: toDouble(VOLUME) //daily stock trading volume
                ::List[Array[String] =>Double]()


    scala.reflect.internal.Positions$ValidateException: Enclosing tree [51025] does not include tree [51024]

    	scala.reflect.internal.Positions$class.positionError$1(Positions.scala:102)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:124)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validatePositions(Positions.scala:147)

    	scala.reflect.internal.SymbolTable.validatePositions(SymbolTable.scala:16)

    	scala.tools.nsc.ast.parser.SyntaxAnalyzer$ParserPhase.apply(SyntaxAnalyzer.scala:102)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$applyPhase$1.apply$mcV$sp(Global.scala:441)

    	scala.tools.nsc.Global$GlobalPhase.withCurrentUnit(Global.scala:432)

    	scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:441)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:399)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:399)

    	scala.collection.Iterator$class.foreach(Iterator.scala:750)

    	scala.collection.AbstractIterator.foreach(Iterator.scala:1202)

    	scala.tools.nsc.Global$GlobalPhase.run(Global.scala:399)

    	scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1500)

    	scala.tools.nsc.Global$Run.compileUnits(Global.scala:1487)

    	scala.tools.nsc.Global$Run.compileSources(Global.scala:1482)

    	scala.tools.nsc.Global$Run.compileFiles(Global.scala:1569)

    	ammonite.interpreter.Compiler$$anon$4.compile(Compiler.scala:195)

    	ammonite.interpreter.Interpreter.evalClass(Interpreter.scala:208)

    	ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:246)

    	ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:244)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:244)

    	ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:243)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter.process(Interpreter.scala:243)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:202)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:201)

    	ammonite.interpreter.Capturing$$anonfun$apply$7.apply(Capture.scala:111)

    	ammonite.interpreter.Capture$$anonfun$ammonite$interpreter$Capture$$withErr$1.apply(Capture.scala:40)

    	scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)

    	scala.Console$.withErr(Console.scala:80)

    	ammonite.interpreter.Capture$.ammonite$interpreter$Capture$$withErr(Capture.scala:36)

    	ammonite.interpreter.Capture$$anonfun$3.apply(Capture.scala:50)

    	ammonite.interpreter.Capture$$anonfun$withOut$1.apply(Capture.scala:31)

    	scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)

    	scala.Console$.withOut(Console.scala:53)

    	ammonite.interpreter.Capture$.withOut(Capture.scala:27)

    	ammonite.interpreter.Capture$.withOutAndErr(Capture.scala:50)

    	ammonite.interpreter.Capture$.apply(Capture.scala:91)

    	ammonite.interpreter.Capturing.apply(Capture.scala:111)

    	ammonite.interpreter.Capturing.flatMap(Capture.scala:116)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:201)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:199)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:199)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:195)

    	ammonite.interpreter.Catching.flatMap(Util.scala:73)

    	ammonite.interpreter.Interpreter.apply(Interpreter.scala:195)

    	jupyter.scala.ScalaInterpreter$$anon$1.interpret(ScalaInterpreter.scala:189)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:82)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)

    	scalaz.concurrent.Task$.Try(Task.scala:379)

    	scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)

    	scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)

    	scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)

    	scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)

    	java.util.concurrent.FutureTask.run(FutureTask.java:266)

    	java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

    	java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

    	java.lang.Thread.run(Thread.java:745)

    Something unexpected went wrong =(



    val xs = DataSource(symbol, path, true) |> extractor //1
    val mv = SimpleMovingAverage(period) //2
    val ratios = xs.map(x => { //3
        val xt = mv get x,toArray
        val zValues = x.drop(period).zip(xt.drop(period))
        zValues.map(z => if(z._1 > z._2) 1 else 0).toArray //4
    })
    var prev = xs(0)(period)
    val label = xs(0).drop(period+1).map( x => { //5
        val y = if( x > prev) 1 else 0
        prev = x; y
    }).toArray
    ratios.transpose.take(label.size).zip(label) //6


    SyntaxError: found ",toArray\n    val zVa", expected ("}" | `case`) in

        val xt = mv get x,toArray

                         ^



    val trainValidRatio = 0.8
    val period = 10
    val labels = XTSeries[(Array[Int], Int)](input.map(x =>
    (x._1.toArray, x._2)).toArray) //7
    val numObsToTrain = (trainValidRatio*labels.size).floor.toInt //8
    val nb = NaiveBayes[Int](labels.take(numObsToTrain)) //9
    validate(labels.drop(numObsForTrains+1), nb) //10


    Compilation Failed

    Main.scala:62: not found: value XTSeries

    XTSeries[(Array[Int], Int)](input.map(x =>

    ^

    Main.scala:62: not found: value input

    XTSeries[(Array[Int], Int)](input.map(x =>

                                ^

    Main.scala:69: not found: value NaiveBayes

    NaiveBayes[Int](labels.take(numObsToTrain)) 

    ^

    Main.scala:72: not found: value validate

    validate(labels.drop(numObsForTrains+1), nb)

    ^


###Results
The next chart plots the value of the F1 measure of the predictor of the direction of the IBM stock using price, volume, and volatility over the previous n trading days, with n varying from 1 to 12 trading days:

![PGM](resultgraph.jpg)

The preceding chart illustrates the impact of the value of the averaging period (number of trading days) on the quality of the multinomial Naïve Bayesian prediction, using the value of stock price, volatility, and volume relative to their average over the averaging period.
From this experiment, we conclude that:

- The prediction of the stock movement using the average price, volume, and volatility is not very good. The F1 measure for the models using weekly (with respect to daily) closing prices varies between 0.68 and 0.74 (with respect to 0.56 and 0.66).
- The prediction using weekly closing prices is more accurate than the prediction using the daily closing prices. In this particular example, the distribution of the weekly closing prices is more reflective of an intermediate term trend than the distribution of daily prices.
- The prediction is somewhat independent of the period used to average the features.

###Multivariate Bernoulli classification
####Model
>###*The Bernoulli mixture model*
![PGM](bern.jpg)


####Implementation
####Naïve Bayes and text mining
- E-mails as legitimate versus spam
- Business news stories
- Movie reviews and scoring
- Technical papers as per field of expertise

####Basics of information retrieval
1. Create or extract the timestamp for each news article.
2. Extract the title, paragraph, and sentences of each article using a Markovian classifier.
3. Extract the terms from each sentence using regular expressions.
4. Correct terms for typos using a dictionary and metric such as the Levenstein distance.
5. Remove the nonstop words.
6. Perform stemming and lemmatization.
7. Extract bags of words and generate a list of n-grams (as a sequence of n terms).
8. Apply a tagging model build using a maximum entropy or conditional random field to extract nouns and adjectives (such as NN, NNP, and so on).
9. Match the terms against a dictionary that supports senses, hyponyms, and synonyms, such as WordNet.
10. Disambiguate word sense using DBpedia [5:12]

#### 정보 검색의 기초
1. 만들거나 각 뉴스 기사에 대한 타임 스탬프를 추출합니다.
2. 마르코프 분류를 사용하여 각 문서의 제목, 단락, 문장의 압축을 풉니 다.
3. 정규 표현식을 사용하여 각 문장의 용어를 추출합니다.
4. 이러한 Levenstein 거리로 사전 및 메트릭을 사용하여 오타를 올바른 용어
5. 논스톱 단어를 제거합니다.
6. 형태소 분석 및 원형 화 수행합니다.
7. 추출 단어의 가방 (N 용어의 순서로) N 그램의 목록을 생성합니다.
8. (등등 같은 NN, NNP 등 등) 명사와 형용사를 추출하는 최대 엔트로피 또는 조건부 랜덤 필드를 사용하여 태그 모델 빌드를 적용합니다.
9. 같은 워드 넷과 같은 감각, hyponyms 및 동의어를 지원하는 사전에 대한 용어를 일치시킵니다.
10. DBpedia를 사용하여 명확하게 단어의 의미 

>###*The Bernoulli mixture model*
![PGM](edit.jpg)


####Implementation
1. Extracting all news with a reference to a specific stock or company in the news feed.
2. Extracting the timestamp or date of the article using a regular expression.
3. Grouping all the news articles related to the stock for a specific date t into a document Dt.
4. Ordering the documents Dt as per the timestamp.
5. Extracting the terms {Ti,D} from each sentence of the document Dt and ranking them by their relative frequency.
6. Aggregating the terms {Tt,i} for all the documents sharing the same release date t.
7. Computing the relative frequency, rtf, of each term, {Tt,i}, for the date t, as the ratio of number of its occurrences in all the articles released at t to the total number of its occurrences of the term in the entire corpus.
8. Normalizing the relative frequency for the average number of articles per date, nrtf.

#### 구현
1. 뉴스 피드에서 특정 주식 또는 회사에 대한 참조를 모든 뉴스를 추출.
2. 정규 표현식을 사용하여 문서의 타임 스탬프 또는 날짜를 추출.
3. 문서 DT로 특정 기간 T에 대한 재고와 관련된 모든 뉴스 기사를 그룹화합니다.
4. 타임 스탬프에 따라 문서 DT 주문.
5. 문서 DT의 각 문장에서 조건 {티, D}를 추출하여 상대 주파수에 의해 그들을 순위.
6. 동일한 릴리스 날짜 (T)를 공유하는 모든 문서에 대한 조항 {TT의 I}를 집계.
7. 용어의 발생의 총 개수를 t 풀어 모든 문서에서의 발생의 수의 비율로서, 기간 t를 들어, {TT의 I} 각 용어의 상대 주파수, RTF 컴퓨팅 전체 코퍼스.
8. nrtf, 날짜 별 기사의 평균 개수에 대한 상대적인 빈도를 정규화.

http://codezip.tistory.com/429 (TF-IDF)

####Extraction of terms
####Scoring of terms
####Testing
####Retrieving textual information
####Evaluation

>###*Bar chart representing predominant keywords in predicting TSLA stock movement*
![PGM](eval.jpg)

The bar chart shows that the terms China, representing all the mentions of the activities of Tesla Motors in China, and Charger, which covers all the references to the charging stations, have a significant positive impact on the direction of the stock with a probability averaging 75 percent. The terms under the category Risk have a negative impact on the direction of the stock with a probability of 68 percent, or a positive impact of the direction of the stock with a probability of 32 percent. Within the remaining eight categories, 72 percent of them were unusable as a predictor of the direction of the stock price.

###Pros and cons
- Simple implementation and easy to parallelize
- Very low computational complexity: O((n+c)*m), where m is the number of features, C the number of classes, and n the number of observations
- Handles missing data
- Supports incremental updates, insertions, and deletions However, Naïve Bayes is not a silver bullet. It has the following disadvantages:
- The assumption of the independence of features is not practical in the real world
- It requires a large training set to achieve reasonable accuracy
- It contains a zero-frequency problem for counters

##Summary
There is a reason why the Naïve Bayes model is the first supervised learning technique you learned: it is simple and robust. As a matter of fact, this is the first technique that should come to mind when you are considering creating a model from a labeled dataset, as long as the features are conditionally independent.

###나이브 베이지안은 계산양이 많고 개념의 뼈대가 되긴하지만 다른 알고리즘을 돌리기 전에 gold standard(기준 평가)가 되는 알고리즘. 물론 Occam's razor(절감의 법칙)에 따라 가장 단순한 가설로서는 모델의 인사이트를 줄 수 있음
