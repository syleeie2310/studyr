
# Scala Study 5 Naive Bayes Classifiers (psygrammer Sangyeol Lee) 08/08(í† )
## Probabilistic graphical models
- Directed graphs such as Bayesian networks
- Undirected graphs such as conditional random fields (refer to the Conditional random fields section in chapter 7, Sequential Data Models)

### Directed graphical models are directed acyclic graphs that have been introduced to:
- Provide a simple way to visualize a probabilistic model
- Describe the conditional dependence (or independence) between variables
- Represent statistical inference in terms of graphical manipulation

>###*Bayes Theorem*
![BayesTheorem](http://www.lancs.ac.uk/~jamest/Group/images/bayesthm.jpg)

>###*Examples of probabilistic graphical models*
![PGM](naive.jpg)

###A Real-world Bayesian network
1. A fire may generate smoke.
2. Smoke may trigger an alarm.
3. A depleted battery may trigger an alarm.
4. The alarm may alert the homeowner.
5. The alarm may alert the fire department.

>###*A Bayesian network for smokre detectors*
![PGM](smoke.jpg)

## Naive Bayes Classifiers
### Introducing the multinomial Naive Bayes
Let's consider the problem of how to predict change in interest rates. The first step is to list the factors that potentially may trigger or cause an increase or decrease in the interest rates. For the sake of illustrating NaÃ¯ve Bayes, we will select the consumer
price index (CPI), change in the Federal fund rate (FDF) and the gross domestic product (GDP) as a first set of features. The terminology is described in the Terminology section under Finances 101 in Appendix A, Basic Concepts.
(ê¸ˆë¦¬ ë³€í™” -> CPI(ì†Œë¹„ì ë¬¼ê°€ì§€ìˆ˜), FDF(ì—°ë°©ê¸°ê¸ˆê¸ˆë¦¬), GDP(êµ­ë‚´ì´ìƒì‚°)

ë‘ê°€ì§€ ê°€ì •
1. ê¸ˆìœµ íˆ¬ì ì „ë¬¸ê°€ê°€ ì—†ìŒ (íˆìŠ¤í† ë¦¬ ë°ì´í„°ì— ì˜ì¡´)
2. 1ë…„ ì¬ë¬´ë¶€ ì¦ê¶Œ ì¦ê°€ì˜ ìˆ˜ìœ¨(ê¸ˆë¦¬ ë³€í™”)ëŠ” ì„ íƒí•œ ê¸°ëŠ¥ê³¼ì˜ ê°•í•œ ìƒê´€ ê´€ê³„ê°€ ìˆìŒ

>###*The Naive Bayes model for predicting the Change in the yield of the 1-year T-bill*
![PGM](t-bill.jpg)

>###*Table*
![PGM](table.jpg)

>###*Table2*
![PGM](table2.jpg)

### Formalism
- Class prior probability or class prior is the probability of a class
- Likelihood is the probability of an observation given a class, also known as the probability of the predictor given a class
- Evidence is the probability of observations occurring, also known as the prior probability of the predictor
- Posterior probability is the probability of an observation x being in a given class

>###*Mathematical notation for the Naive Bayes model*
![PGM](formal.jpg)

>###*Frequentist perspective*
![PGM](result.jpg)

####This problem is not a good candidate for a Bayesian classification for two reasons:
- The training set is not large enough to compute accurate prior probabilities and generate a stable model; decades of quarterly GDP data is needed to train and validate the model
- The features have different rates of change, which predominately favor the feature with the highest frequency; in this case, the CPI


###The predictive model
- The closing price, pr(t), of the last trading session, t, is above or below the average closing price over the n previous trading days, [t-n, t]
- The volume of the last trading day, vl(t), is above or below the average volume of the n previous trading days
- The volatility on the last trading day, vt(t), is above or below the average volatility of the previous n trading days

![PGM](stock.jpg)

### The zero-frquency problem
![PGM](zero.jpg)


### Implementation
#### Software design
- A generic model, NaiveBayesModel, of the type Model, which is initialized through training during the instantiation of the class.
- A model for the binomial classification, BinNaiveBayesModel, which subclasses NaiveBayesModel. The model consists of a density function of the type Density, and a pair of positive and negative Likelihood instances. 
- A model for the multinomial classification MultiNaiveBayesModel.
- The predictive or classification routine is implemented as a data transformation extending the PipeOperator trait.
- The NaiveBayes classifier class has two parameters: a smoothing

>###*UML Class diagram*
![PGM](umlnaive.jpg)

###Training
- The number of occurrences k of this features for N > k observations in case of binary features or counters
- The mean value for all the observations for this features in the case of numeric or continuous features
- ê°€ì • : ê°€ê²©, ë³¼ë¥¨, ë³€ë™ì„±ì´ ì¡°ê±´ë¶€ ë…ë¦½ì„ì„ ê°€ì •

####Likelihood class
- The label for the observation, label
- An array of tuple Laplace or Lidstone smoothed mean and standard deviation, muSigma
- The prior class prior that computes p(c)


    type Density = (Double*) => Double //1
    type XYTSeries = Array[(Double, Double)]
    val MINLOGARG = 1e-32
    val MINLOGVALUE = -MINLOGARG
    class Likelihood[T <% Double](val label: Int, val muSigma: XYTSeries,
    prior: Double) { //2
        def score(obs: Array[T], density: Density): Double =
         (obs, muSigma).zipped
                         .foldLeft(0.0)((post, xms) => {
                         val mean = xms._2._1
                         val stdDev = xms._2._2
                         val _obs = xms._1
            val prob = density(mean, stdDev, _obs)
            post + Math.log(if(prob< MINLOGARG) MINLOGVALUE else prob)
        }) + Math.log(prior) //3
    }


    defined [32mtype [36mDensity[0m
    defined [32mtype [36mXYTSeries[0m
    [36mMINLOGARG[0m: [32mDouble[0m = [32m1.0E-32[0m
    [36mMINLOGVALUE[0m: [32mDouble[0m = [32m-1.0E-32[0m
    defined [32mclass [36mLikelihood[0m


####The parameterized, view-bounded class, Likelihood, has two purposes:
- Define the model extracted from training (likelihood for each feature and the class prior) in the constructor (line 2)
- Compute the score of a new observation as part of the classification process score (line 3). The computation of the log of the likelihood uses a density method of the type Density, which is an argument of the score method. As seen in the next section, the density can be either a Gaussian or a Bernoulli distribution. The score method uses the Scala's zipped method to merge the observation values with the labeled output

####ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœì„œìŒìœ¼ë¡œ ë¬¶ê¸°: zip, unzip
- zip ë‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¸ìë¡œ ë°›ì•„ì„œ ìˆœì„œìŒì˜ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ë©°, ë§Œì•½ ì¸ì ë¦¬ìŠ¤íŠ¸ ë‘˜ì˜ ê¸¸ì´ê°€ ë‹¤ë¥´ë‹¤ë©´ ê¸¸ì´ê°€ ê¸´ ìª½ì˜ ë‚¨ëŠ” ì›ì†ŒëŠ” ë²„ë¦°ë‹¤.

The next step is to define the BinNaiveBayesModel model for a two-class classification scheme. The two-class model consists of the two Likelihood instances:

positives for the label UP (value==1) and negatives for the label DOWN (value==0). In order to make the model generic, we created NaiveBayesModel, an abstract class that can be extended as needed to support both the Binomial and Multinomial NaÃ¯ve Bayes models, as follows:


    val abcde = List('a', 'b', 'c', 'd', 'e')
    abcde.indices zip abcde
    val zipped = abcde zip List(1, 2, 3)


    [36mabcde[0m: [32mList[0m[[32mChar[0m] = [33mList[0m([32m'a'[0m, [32m'b'[0m, [32m'c'[0m, [32m'd'[0m, [32m'e'[0m)
    [36mres1_1[0m: [32mcollection[0m.[32mimmutable[0m.[32mIndexedSeq[0m[([32mInt[0m, [32mChar[0m)] = [33mVector[0m(
      [33m[0m([32m0[0m, [32m'a'[0m),
      [33m[0m([32m1[0m, [32m'b'[0m),
      [33m[0m([32m2[0m, [32m'c'[0m),
      [33m[0m([32m3[0m, [32m'd'[0m),
      [33m[0m([32m4[0m, [32m'e'[0m)
    )
    [36mzipped[0m: [32mList[0m[([32mChar[0m, [32mInt[0m)] = [33mList[0m(
      [33m[0m([32m'a'[0m, [32m1[0m),
      [33m[0m([32m'b'[0m, [32m2[0m),
      [33m[0m([32m'c'[0m, [32m3[0m)
    )



    type XY = (Double, Double)
    type XYTSeries = Array[(Double, Double)]
    type DMatrix[T] = Array[Array[T]]
    type DVector[T] = Array[T]
    type DblMatrix = DMatrix[Double]
    type DblVector = Array[Double]


    defined [32mtype [36mXY[0m
    defined [32mtype [36mXYTSeries[0m
    defined [32mtype [36mDMatrix[0m
    defined [32mtype [36mDVector[0m
    defined [32mtype [36mDblMatrix[0m
    defined [32mtype [36mDblVector[0m



    class XTSeries[T](label: String, arr: Array[T])


    defined [32mclass [36mXTSeries[0m



    class XTSeries[T](label: String, arr: Array[T]) { // 1
      def apply(n: Int): T = arr.apply(n)
        
      @implicitNotFound("Undefined conversion to DblVector") // 2
      def toDblVector(implicit f: T=>Double):DblVector =arr.map(f(_))
    
      @implicitNotFound("Undefined conversion to DblMatrix") // 2
      def toDblMatrix(implicit fv: T => DblVector): DblMatrix = arr.map(
    fv( _ ) )
    
      def + (n: Int, t: T)(implicit f: (T,T) => T): T = f(arr(n), t)
    
      def head: T = arr.head //3
      def drop(n: Int):XTSeries[T] = XTSeries(label,arr.drop(n))
      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,
    arr.map( x =>f(x)))
      def foreach( f: T => Unit) = arr.foreach(f) //3
      def sortWith(lt: (T,T)=>Boolean):XTSeries[T] = XTSeries[T](label,
    arr.sortWith(lt))
      def max(implicit cmp: Ordering[T]): T = arr.max //4
    def min(implicit cmp: Ordering[T]): T = arr.min
    }


    Compilation Failed

    Main.scala:76: not found: type implicitNotFound

      @implicitNotFound("Undefined conversion to DblVector") // 2

       ^

    Main.scala:79: not found: type implicitNotFound

      @implicitNotFound("Undefined conversion to DblMatrix") // 2

       ^

    Main.scala:86: not found: value XTSeries

      def drop(n: Int):XTSeries[T] = XTSeries(label,arr.drop(n))

                                     ^

    Main.scala:87: not found: type ClassTag

      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,

                 ^

    Main.scala:87: not found: value XTSeries

      def map[U: ClassTag](f: T => U): XTSeries[U] = XTSeries[U](label,

                                                     ^

    Main.scala:90: not found: value XTSeries

      def sortWith(lt: (T,T)=>Boolean):XTSeries[T] = XTSeries[T](label,

                                                     ^



    val numFeatures = 4


    [36mnumFeatures[0m: [32mInt[0m = [32m4[0m



    abstract class NaiveBayesModel [T <% Double](density: Density) {
        def classify(values: DblVector): Int
    }


    defined [32mclass [36mNaiveBayesModel[0m



    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 
    {
        override def classify(x: Array[T]): Int = if (positives.score(x,density) > negatives.score(x,density)) 1 else 0
    }


    Compilation Failed

    Main.scala:81: class Likelihood takes type parameters

                    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 

                                                                      ^

    Main.scala:81: class Likelihood takes type parameters

                    class BinNaiveBayesModel [T <% Double](positives: Likelihood, negatives: Likelihood, density: Density) extends NaiveBayesModel [T](density) 

                                                                                             ^


###multinomial Naive Bayes model
The multinomial NaÃ¯ve Bayes model differs from the binomial version in the following ways:
- The likelihood is defined as a list, likelihoodXs (one likelihood per class)
- The runtime classification sorts the class by the log likelihood (sortWith), selects the class with the highest score, and returns the class ID

The validate method takes a labeled time series xt as an array of tuples (observation, class label) and the tpClass index that contains the true positives (that is, increase in the stock price) outcome. The method returns an F1-measure. Besides inheriting the Supervised trait, the NaiveBayes class inherits the PipeOperator trait so that it can be integrated into a generic workflow as one of the computation units.

The attributes of the multinomial NaÃ¯ve Bayes are as follows:
- The smoothing formula (Laplace, Lidstone, and so on): smoothing
- The labeled training set defined as a time series: xt
- The probability density function: density

>###*F1-Measure*
![PGM](f1.jpg)

http://blog.naver.com/kmkim1222?Redirect=Log&logNo=220106232149 (í˜¼ë™í–‰ë ¬)


    class MultiNaiveBayesModel[T <% Double](likelihoodXs:List[Likelihood[T]], density: Density) extends NaiveBayesModel[T](density) 
    {
        override def classify(x: Array[T]): Int = likelihoodXs.sortWith( (p1,p2) => p1.score(x, density) >
    p2.score(x, density)).head.label
    }


    Compilation Failed

    Main.scala:81: class MultiNaiveBayesModel needs to be abstract, since method classify in class NaiveBayesModel of type (values: cmd6.INSTANCE.$ref$cmd3.DblVector)Int is not defined

    (Note that cmd6.this.$ref$cmd3.DblVector does not match Array[T]: their type parameters differ)

                    class MultiNaiveBayesModel[T <% Double](likelihoodXs:List[Likelihood[T]], density: Density) extends NaiveBayesModel[T](density) 

                          ^

    Main.scala:83: method classify overrides nothing.

    Note: the super classes of class MultiNaiveBayesModel contain the following, non final members named classify:

    def classify(values: cmd6.INSTANCE.$ref$cmd3.DblVector): Int

        override def classify(x: Array[T]): Int = likelihoodXs.sortWith( (p1,p2) => p1.score(x, density) >

                     ^



    trait Supervised[T] {
        def validate(xt: XTSeries[(Array[T],Int)], tpClass:Int): Double
    }


    defined [32mtrait [36mSupervised[0m



    Class NaiveBayes[T <% Double](smoothing: Double, xt:XTSeries[(Array[T], Int)], density: Density) extends PipeOperator[XTSeries[Array[T]], Array[Int]] with Supervised[T] 
    {
        val model = BinNaiveBayesModel[T](train(1),train(0),density) //1
        def train(label:Int)(implicit f: Array[T] => DblVector):Likelihood[T] = { //2
            val xi = xt.toArray
            val values= xi.filter( _._2 == label).map(x => f(x._1) )
            val dim = xi(0)._1.size
            val vt = XTSeries[DblVector](values.toArray) //3
            val muStdDev = statistics(vt).map(stat => (stat.lidstoneMean(smoothing, dim), stat.stdDev)) Likelihood(label, muStdDev, values.size.toDouble/xi.size) //4
    }


    SyntaxError: found " <% Double](smoothin", expected ("," ~ Type | "]") in

    Class NaiveBayes[T <% Double](smoothing: Double, xt:XTSeries[(Array[T], Int)], density: Density) extends PipeOperator[XTSeries[Array[T]], Array[Int]] with Supervised[T] 

                      ^


###Classification
The likelihood and class prior that have been computed through training is used for validating the model and classifying new observations.

The score represents the log of likelihood estimate (or the posterior probability), which is computed as the summation of the log of the Gaussian distribution using the mean and standard deviation, extracted from the training phase and the log of the likelihood class.

The NaÃ¯ve Bayes classification using Gaussian distribution is illustrated using two classes, C1 and C2, and a model with two features (x and y):

![PGM](gaussian.jpg)
>###*NaÃ¯ve Bayes classification using Gaussian density*
![PGM](gaussian2.jpg)

https://ko.wikipedia.org/wiki/%EB%82%98%EC%9D%B4%EB%B8%8C_%EB%B2%A0%EC%9D%B4%EC%A6%88_%EB%B6%84%EB%A5%98 (ìœ„í‚¤í”¼ë””ì•„ ë‚˜ì´ë¸Œ ë² ì´ì§€ ë¶„ë¥˜)


    object Stats {
        final val INV_SQRT_2PI = 1.0/Math.sqrt(2.0*Math.PI)
        def gauss(mu: Double, sigma: Double, x:Double) : Double = {
         val y = x - mu
         INV_SQRT_2PI/sigma * Math.exp(-0.5*y*y/sigma*sigma)
    }
    def gauss(x: Double*): Double = gauss(x(0), x(1), x(2))
    }


    defined [32mobject [36mStats[0m


Finally, the classification method is implemented as the pipe operator |> of the NaiveBayes class. The classification model and the density function are provided at runtime as attributes of the class:


    def |> : PartialFunction[XTSeries[Array[T]], Array[Int]] = {
        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}


    Compilation Failed

    Main.scala:80: not found: type T

                    def |> : PartialFunction[XTSeries[Array[T]], Array[Int]] = {

                                                            ^

    Main.scala:81: not found: type T

        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}

                                ^

    Main.scala:81: not found: value model

        case xt: XTSeries[Array[T]] if(xt != null && xt.size > 0 && model !=None) => xt.toArray.map( model.classify( _))}

                                                                    ^


###Labeling
The most critical element in the training of a supervised learning algorithm is the creation of labeled data. Fortunately, in this case, the label (or expected class) can be automatically generated. The objective is to predict the direction of the price of a stock for the next trading day, taking into account the average price, volume, and volatility over the last n days.

ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì¼ì¼ ë° ì£¼ê°„ ì¢…ê°€ì™€ 2000ë…„ 1ì›” 1ì¼ ë° 2014ë…„ 12ì›” 31ì¼ì˜ ê¸°ê°„ ë™ì•ˆ ê° ì£¼ì‹ì˜ í‰ê·  ê°€ê²©, ë³¼ë¥¨ ë° ë³€ë™ì„±ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì œ [TN, T] ì°½ ì´ëŸ¬í•œ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë‹¨ìˆœ ì´ë™ í‰ê· ì„ ì‚¬ìš©í•˜ì. ë‹¤ìŒê³¼ ê°™ì´ ì²«ì§¸, ì¶”ì¶œ ê¸°ëŠ¥ì€ toDoubleì™€ % ì—°ì‚°ìë¥¼ ë°ì´í„° ì¶”ì¶œì— ì„¤ëª… ë° ë°ì´í„°ëŠ” ë¶€ë¡ A, ê¸°ë³¸ ê°œë… ì„¹ì…˜ì—ì„œ ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ê±°ë˜ì¼ì˜ ì¢…ê°€, ë†’ì€, ë‚®ì€ ê°€ê²©, ê·¸ë¦¬ê³  ë³¼ë¥¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤

- Extractor ((closing, high, low) prices and volume for each trading day)
- Window period ë”°ë¥¸ ë‹¨ìˆœí‰ê· ì´ë™ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê° ì£¼ì‹ì˜ í†µê³„ëŸ‰ì„ ë½‘ìŒ


    val numbers = List(1, 2, 3, 4)
    numbers.drop(2)


    [36mnumbers[0m: [32mList[0m[[32mInt[0m] = [33mList[0m([32m1[0m, [32m2[0m, [32m3[0m, [32m4[0m)
    [36mres10_1[0m: [32mList[0m[[32mInt[0m] = [33mList[0m([32m3[0m, [32m4[0m)



    val extractor = toDouble(CLOSE) //stock closing price
                :: ratio(HIGH, LOW) //volatility (HIGH-LOW)/HIGH
                :: toDouble(VOLUME) //daily stock trading volume
                ::List[Array[String] =>Double]()


    scala.reflect.internal.Positions$ValidateException: Enclosing tree [51025] does not include tree [51024]

    	scala.reflect.internal.Positions$class.positionError$1(Positions.scala:102)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:124)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validate$1(Positions.scala:142)

    	scala.reflect.internal.Positions$class.validatePositions(Positions.scala:147)

    	scala.reflect.internal.SymbolTable.validatePositions(SymbolTable.scala:16)

    	scala.tools.nsc.ast.parser.SyntaxAnalyzer$ParserPhase.apply(SyntaxAnalyzer.scala:102)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$applyPhase$1.apply$mcV$sp(Global.scala:441)

    	scala.tools.nsc.Global$GlobalPhase.withCurrentUnit(Global.scala:432)

    	scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:441)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:399)

    	scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:399)

    	scala.collection.Iterator$class.foreach(Iterator.scala:750)

    	scala.collection.AbstractIterator.foreach(Iterator.scala:1202)

    	scala.tools.nsc.Global$GlobalPhase.run(Global.scala:399)

    	scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1500)

    	scala.tools.nsc.Global$Run.compileUnits(Global.scala:1487)

    	scala.tools.nsc.Global$Run.compileSources(Global.scala:1482)

    	scala.tools.nsc.Global$Run.compileFiles(Global.scala:1569)

    	ammonite.interpreter.Compiler$$anon$4.compile(Compiler.scala:195)

    	ammonite.interpreter.Interpreter.evalClass(Interpreter.scala:208)

    	ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:246)

    	ammonite.interpreter.Interpreter$$anonfun$process$1$$anonfun$apply$21.apply(Interpreter.scala:244)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:244)

    	ammonite.interpreter.Interpreter$$anonfun$process$1.apply(Interpreter.scala:243)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter.process(Interpreter.scala:243)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:202)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9$$anonfun$apply$10.apply(Interpreter.scala:201)

    	ammonite.interpreter.Capturing$$anonfun$apply$7.apply(Capture.scala:111)

    	ammonite.interpreter.Capture$$anonfun$ammonite$interpreter$Capture$$withErr$1.apply(Capture.scala:40)

    	scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)

    	scala.Console$.withErr(Console.scala:80)

    	ammonite.interpreter.Capture$.ammonite$interpreter$Capture$$withErr(Capture.scala:36)

    	ammonite.interpreter.Capture$$anonfun$3.apply(Capture.scala:50)

    	ammonite.interpreter.Capture$$anonfun$withOut$1.apply(Capture.scala:31)

    	scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)

    	scala.Console$.withOut(Console.scala:53)

    	ammonite.interpreter.Capture$.withOut(Capture.scala:27)

    	ammonite.interpreter.Capture$.withOutAndErr(Capture.scala:50)

    	ammonite.interpreter.Capture$.apply(Capture.scala:91)

    	ammonite.interpreter.Capturing.apply(Capture.scala:111)

    	ammonite.interpreter.Capturing.flatMap(Capture.scala:116)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:201)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6$$anonfun$apply$9.apply(Interpreter.scala:199)

    	ammonite.interpreter.Res$Success.flatMap(Util.scala:27)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:199)

    	ammonite.interpreter.Interpreter$$anonfun$apply$6.apply(Interpreter.scala:195)

    	ammonite.interpreter.Catching.flatMap(Util.scala:73)

    	ammonite.interpreter.Interpreter.apply(Interpreter.scala:195)

    	jupyter.scala.ScalaInterpreter$$anon$1.interpret(ScalaInterpreter.scala:189)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:100)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$execute$1$$anonfun$apply$6.apply(InterpreterHandler.scala:82)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)

    	jupyter.kernel.interpreter.InterpreterHandler$$anonfun$jupyter$kernel$interpreter$InterpreterHandler$$publishing$1$$anonfun$4.apply(InterpreterHandler.scala:55)

    	scalaz.concurrent.Task$.Try(Task.scala:379)

    	scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)

    	scalaz.concurrent.Task$$anonfun$unsafeStart$1.apply(Task.scala:290)

    	scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)

    	scalaz.concurrent.Future$$anonfun$apply$15$$anon$3.call(Future.scala:367)

    	java.util.concurrent.FutureTask.run(FutureTask.java:266)

    	java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)

    	java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)

    	java.lang.Thread.run(Thread.java:745)

    Something unexpected went wrong =(



    val xs = DataSource(symbol, path, true) |> extractor //1
    val mv = SimpleMovingAverage(period) //2
    val ratios = xs.map(x => { //3
        val xt = mv get x,toArray
        val zValues = x.drop(period).zip(xt.drop(period))
        zValues.map(z => if(z._1 > z._2) 1 else 0).toArray //4
    })
    var prev = xs(0)(period)
    val label = xs(0).drop(period+1).map( x => { //5
        val y = if( x > prev) 1 else 0
        prev = x; y
    }).toArray
    ratios.transpose.take(label.size).zip(label) //6


    SyntaxError: found ",toArray\n    val zVa", expected ("}" | `case`) in

        val xt = mv get x,toArray

                         ^



    val trainValidRatio = 0.8
    val period = 10
    val labels = XTSeries[(Array[Int], Int)](input.map(x =>
    (x._1.toArray, x._2)).toArray) //7
    val numObsToTrain = (trainValidRatio*labels.size).floor.toInt //8
    val nb = NaiveBayes[Int](labels.take(numObsToTrain)) //9
    validate(labels.drop(numObsForTrains+1), nb) //10


    Compilation Failed

    Main.scala:62: not found: value XTSeries

    XTSeries[(Array[Int], Int)](input.map(x =>

    ^

    Main.scala:62: not found: value input

    XTSeries[(Array[Int], Int)](input.map(x =>

                                ^

    Main.scala:69: not found: value NaiveBayes

    NaiveBayes[Int](labels.take(numObsToTrain)) 

    ^

    Main.scala:72: not found: value validate

    validate(labels.drop(numObsForTrains+1), nb)

    ^


###Results
The next chart plots the value of the F1 measure of the predictor of the direction of the IBM stock using price, volume, and volatility over the previous n trading days, with n varying from 1 to 12 trading days:

![PGM](resultgraph.jpg)

The preceding chart illustrates the impact of the value of the averaging period (number of trading days) on the quality of the multinomial NaÃ¯ve Bayesian prediction, using the value of stock price, volatility, and volume relative to their average over the averaging period.
From this experiment, we conclude that:

- The prediction of the stock movement using the average price, volume, and volatility is not very good. The F1 measure for the models using weekly (with respect to daily) closing prices varies between 0.68 and 0.74 (with respect to 0.56 and 0.66).
- The prediction using weekly closing prices is more accurate than the prediction using the daily closing prices. In this particular example, the distribution of the weekly closing prices is more reflective of an intermediate term trend than the distribution of daily prices.
- The prediction is somewhat independent of the period used to average the features.

###Multivariate Bernoulli classification
####Model
>###*The Bernoulli mixture model*
![PGM](bern.jpg)


####Implementation
####NaÃ¯ve Bayes and text mining
- E-mails as legitimate versus spam
- Business news stories
- Movie reviews and scoring
- Technical papers as per field of expertise

####Basics of information retrieval
1. Create or extract the timestamp for each news article.
2. Extract the title, paragraph, and sentences of each article using a Markovian classifier.
3. Extract the terms from each sentence using regular expressions.
4. Correct terms for typos using a dictionary and metric such as the Levenstein distance.
5. Remove the nonstop words.
6. Perform stemming and lemmatization.
7. Extract bags of words and generate a list of n-grams (as a sequence of n terms).
8. Apply a tagging model build using a maximum entropy or conditional random field to extract nouns and adjectives (such as NN, NNP, and so on).
9. Match the terms against a dictionary that supports senses, hyponyms, and synonyms, such as WordNet.
10. Disambiguate word sense using DBpedia [5:12]

#### ì •ë³´ ê²€ìƒ‰ì˜ ê¸°ì´ˆ
1. ë§Œë“¤ê±°ë‚˜ ê° ë‰´ìŠ¤ ê¸°ì‚¬ì— ëŒ€í•œ íƒ€ì„ ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
2. ë§ˆë¥´ì½”í”„ ë¶„ë¥˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì„œì˜ ì œëª©, ë‹¨ë½, ë¬¸ì¥ì˜ ì••ì¶•ì„ í’‰ë‹ˆ ë‹¤.
3. ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê° ë¬¸ì¥ì˜ ìš©ì–´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
4. ì´ëŸ¬í•œ Levenstein ê±°ë¦¬ë¡œ ì‚¬ì „ ë° ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ ì˜¤íƒ€ë¥¼ ì˜¬ë°”ë¥¸ ìš©ì–´
5. ë…¼ìŠ¤í†± ë‹¨ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
6. í˜•íƒœì†Œ ë¶„ì„ ë° ì›í˜• í™” ìˆ˜í–‰í•©ë‹ˆë‹¤.
7. ì¶”ì¶œ ë‹¨ì–´ì˜ ê°€ë°© (N ìš©ì–´ì˜ ìˆœì„œë¡œ) N ê·¸ë¨ì˜ ëª©ë¡ì„ ìƒì„±í•©ë‹ˆë‹¤.
8. (ë“±ë“± ê°™ì€ NN, NNP ë“± ë“±) ëª…ì‚¬ì™€ í˜•ìš©ì‚¬ë¥¼ ì¶”ì¶œí•˜ëŠ” ìµœëŒ€ ì—”íŠ¸ë¡œí”¼ ë˜ëŠ” ì¡°ê±´ë¶€ ëœë¤ í•„ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ íƒœê·¸ ëª¨ë¸ ë¹Œë“œë¥¼ ì ìš©í•©ë‹ˆë‹¤.
9. ê°™ì€ ì›Œë“œ ë„·ê³¼ ê°™ì€ ê°ê°, hyponyms ë° ë™ì˜ì–´ë¥¼ ì§€ì›í•˜ëŠ” ì‚¬ì „ì— ëŒ€í•œ ìš©ì–´ë¥¼ ì¼ì¹˜ì‹œí‚µë‹ˆë‹¤.
10. DBpediaë¥¼ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•˜ê²Œ ë‹¨ì–´ì˜ ì˜ë¯¸ 

>###*The Bernoulli mixture model*
![PGM](edit.jpg)


####Implementation
1. Extracting all news with a reference to a specific stock or company in the news feed.
2. Extracting the timestamp or date of the article using a regular expression.
3. Grouping all the news articles related to the stock for a specific date t into a document Dt.
4. Ordering the documents Dt as per the timestamp.
5. Extracting the terms {Ti,D} from each sentence of the document Dt and ranking them by their relative frequency.
6. Aggregating the terms {Tt,i} for all the documents sharing the same release date t.
7. Computing the relative frequency, rtf, of each term, {Tt,i}, for the date t, as the ratio of number of its occurrences in all the articles released at t to the total number of its occurrences of the term in the entire corpus.
8. Normalizing the relative frequency for the average number of articles per date, nrtf.

#### êµ¬í˜„
1. ë‰´ìŠ¤ í”¼ë“œì—ì„œ íŠ¹ì • ì£¼ì‹ ë˜ëŠ” íšŒì‚¬ì— ëŒ€í•œ ì°¸ì¡°ë¥¼ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ì¶”ì¶œ.
2. ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œì˜ íƒ€ì„ ìŠ¤íƒ¬í”„ ë˜ëŠ” ë‚ ì§œë¥¼ ì¶”ì¶œ.
3. ë¬¸ì„œ DTë¡œ íŠ¹ì • ê¸°ê°„ Tì— ëŒ€í•œ ì¬ê³ ì™€ ê´€ë ¨ëœ ëª¨ë“  ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.
4. íƒ€ì„ ìŠ¤íƒ¬í”„ì— ë”°ë¼ ë¬¸ì„œ DT ì£¼ë¬¸.
5. ë¬¸ì„œ DTì˜ ê° ë¬¸ì¥ì—ì„œ ì¡°ê±´ {í‹°, D}ë¥¼ ì¶”ì¶œí•˜ì—¬ ìƒëŒ€ ì£¼íŒŒìˆ˜ì— ì˜í•´ ê·¸ë“¤ì„ ìˆœìœ„.
6. ë™ì¼í•œ ë¦´ë¦¬ìŠ¤ ë‚ ì§œ (T)ë¥¼ ê³µìœ í•˜ëŠ” ëª¨ë“  ë¬¸ì„œì— ëŒ€í•œ ì¡°í•­ {TTì˜ I}ë¥¼ ì§‘ê³„.
7. ìš©ì–´ì˜ ë°œìƒì˜ ì´ ê°œìˆ˜ë¥¼ t í’€ì–´ ëª¨ë“  ë¬¸ì„œì—ì„œì˜ ë°œìƒì˜ ìˆ˜ì˜ ë¹„ìœ¨ë¡œì„œ, ê¸°ê°„ të¥¼ ë“¤ì–´, {TTì˜ I} ê° ìš©ì–´ì˜ ìƒëŒ€ ì£¼íŒŒìˆ˜, RTF ì»´í“¨íŒ… ì „ì²´ ì½”í¼ìŠ¤.
8. nrtf, ë‚ ì§œ ë³„ ê¸°ì‚¬ì˜ í‰ê·  ê°œìˆ˜ì— ëŒ€í•œ ìƒëŒ€ì ì¸ ë¹ˆë„ë¥¼ ì •ê·œí™”.

http://codezip.tistory.com/429 (TF-IDF)

####Extraction of terms
####Scoring of terms
####Testing
####Retrieving textual information
####Evaluation

>###*Bar chart representing predominant keywords in predicting TSLA stock movement*
![PGM](eval.jpg)

The bar chart shows that the terms China, representing all the mentions of the activities of Tesla Motors in China, and Charger, which covers all the references to the charging stations, have a significant positive impact on the direction of the stock with a probability averaging 75 percent. The terms under the category Risk have a negative impact on the direction of the stock with a probability of 68 percent, or a positive impact of the direction of the stock with a probability of 32 percent. Within the remaining eight categories, 72 percent of them were unusable as a predictor of the direction of the stock price.

###Pros and cons
- Simple implementation and easy to parallelize
- Very low computational complexity: O((n+c)*m), where m is the number of features, C the number of classes, and n the number of observations
- Handles missing data
- Supports incremental updates, insertions, and deletions However, NaÃ¯ve Bayes is not a silver bullet. It has the following disadvantages:
- The assumption of the independence of features is not practical in the real world
- It requires a large training set to achieve reasonable accuracy
- It contains a zero-frequency problem for counters

##Summary
There is a reason why the NaÃ¯ve Bayes model is the first supervised learning technique you learned: it is simple and robust. As a matter of fact, this is the first technique that should come to mind when you are considering creating a model from a labeled dataset, as long as the features are conditionally independent.

###ë‚˜ì´ë¸Œ ë² ì´ì§€ì•ˆì€ ê³„ì‚°ì–‘ì´ ë§ê³  ê°œë…ì˜ ë¼ˆëŒ€ê°€ ë˜ê¸´í•˜ì§€ë§Œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ ëŒë¦¬ê¸° ì „ì— gold standard(ê¸°ì¤€ í‰ê°€)ê°€ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜. ë¬¼ë¡  Occam's razor(ì ˆê°ì˜ ë²•ì¹™)ì— ë”°ë¼ ê°€ì¥ ë‹¨ìˆœí•œ ê°€ì„¤ë¡œì„œëŠ” ëª¨ë¸ì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¤„ ìˆ˜ ìˆìŒ
